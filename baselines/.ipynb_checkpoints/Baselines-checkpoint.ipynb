{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e05559b",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "This notebook prepocesses the hypergraphs so the baselines can learn them. For the diffusion / GAN / VAE it converts the hypergraphs to padded images where a white pixel is a 1 and a black pixel is a 0.\n",
    "\n",
    "For HyperPA, it outputs approximated edge size distribution and number of new hyperedges introduced by each node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eafb0c",
   "metadata": {},
   "source": [
    "## Subset Sampling (unfinished baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1145be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "# Load the .pkl file\n",
    "dataset_name = \"hypergraphErdosRenyi\"\n",
    "pkl_filename = \"../data/\" + dataset_name + \".pkl\"\n",
    "with open(pkl_filename, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Create the output file\n",
    "output_filename = f\"{dataset_name}-seqs.txt\"\n",
    "\n",
    "with open(output_filename, 'w') as out_file:\n",
    "    # Process hypergraphs in the 'train' key\n",
    "    for hnx_hypergraph in data['train']:\n",
    "        # Get all hyperedges\n",
    "        hyperedges = hnx_hypergraph.edges\n",
    "        \n",
    "        # Repeat the process multiple times for each hypergraph\n",
    "        for _ in range(5):  # Adjust the number of repetitions as needed\n",
    "            # Randomize the order of hyperedges\n",
    "            keys = list(hyperedges)\n",
    "            random.shuffle(keys)\n",
    "            \n",
    "            # Prepare the line data\n",
    "            sizes = []\n",
    "            elements = []\n",
    "            \n",
    "            for key in keys:\n",
    "                sizes.append(len(hyperedges[key]))\n",
    "                elements.extend(hyperedges[key])\n",
    "            \n",
    "            # Create the line string\n",
    "            line = f\"{','.join(map(str, sizes))};{','.join(map(str, elements))}\\n\"\n",
    "            \n",
    "            # Write the line to the output file\n",
    "            out_file.write(line)\n",
    "\n",
    "print(f\"Output written to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879aa5f8",
   "metadata": {},
   "source": [
    "## HyperPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262dd298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import hypernetx as hnx\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Define the directory paths\n",
    "dataset_dir = '../data'\n",
    "output_dir_simplex = 'hyperpa_SS/simplex per node'\n",
    "output_dir_size_distribution = 'hyperpa_SS/size distribution'\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_dir_simplex, exist_ok=True)\n",
    "os.makedirs(output_dir_size_distribution, exist_ok=True)\n",
    "\n",
    "def write_simplex_per_node_distribution(filename, hypergraph):\n",
    "    n = len(hypergraph.nodes)\n",
    "    m = len(hypergraph.edges)\n",
    "    \n",
    "    # Initialize distribution\n",
    "    distribution = [0] * n\n",
    "    \n",
    "    remaining_edges = m\n",
    "    nodes_to_attribute = min(m, n)\n",
    "    \n",
    "    while remaining_edges > 0:\n",
    "        for i in range(nodes_to_attribute):\n",
    "            distribution[i] += 1\n",
    "        \n",
    "        remaining_edges -= nodes_to_attribute\n",
    "        nodes_to_attribute = min(remaining_edges, n)\n",
    "    \n",
    "    # Count the frequency of each number of attributed edges\n",
    "    max_edges = max(distribution)\n",
    "    counts = [distribution.count(i) for i in range(max_edges + 1)]\n",
    "    \n",
    "    # Convert counts to percentages and round\n",
    "    percentages = [round(count / n * 100) for count in counts]\n",
    "    \n",
    "    # Ensure the sum is exactly 100%\n",
    "    while sum(percentages) != 100:\n",
    "        if sum(percentages) < 100:\n",
    "            percentages[percentages.index(max(percentages))] += 1\n",
    "        else:\n",
    "            percentages[percentages.index(min(filter(lambda x: x != 0, percentages)))] -= 1\n",
    "    \n",
    "    # Write the distribution to file\n",
    "    with open(filename, 'w') as f:\n",
    "        for percentage in percentages:\n",
    "            f.write(f\"{percentage}\\n\")\n",
    "\n",
    "def write_size_distribution(filename, hypergraph):\n",
    "    size_distribution = np.zeros(25)\n",
    "    for hyperedge in hypergraph.edges:\n",
    "        size = len(hypergraph.edges[hyperedge])\n",
    "        if size <= 25:  # Ensure we don't exceed array bounds\n",
    "            size_distribution[size-1] += 1\n",
    "    \n",
    "    # Convert counts to percentages\n",
    "    total_edges = sum(size_distribution)\n",
    "    percentages = [round(count / total_edges * 100) for count in size_distribution]\n",
    "    \n",
    "    # Ensure the sum is exactly 100%\n",
    "    while sum(percentages) != 100:\n",
    "        if sum(percentages) < 100:\n",
    "            percentages[percentages.index(max(percentages))] += 1\n",
    "        else:\n",
    "            percentages[percentages.index(min(filter(lambda x: x != 0, percentages)))] -= 1\n",
    "    \n",
    "    # Write the distribution to file\n",
    "    with open(filename, 'w') as f:\n",
    "        for percentage in percentages:\n",
    "            f.write(f\"{percentage}\\n\")\n",
    "\n",
    "# Iterate over each .pkl file in the dataset directory\n",
    "for file in os.listdir(dataset_dir):\n",
    "    if file.endswith('.pkl'):\n",
    "        file_path = os.path.join(dataset_dir, file)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            hypergraphs = pickle.load(f)\n",
    "\n",
    "        # Process the specified hypergraph in each file\n",
    "        for i in range(len(hypergraphs['test'])):\n",
    "            hypergraph = hypergraphs['test'][i]\n",
    "            \n",
    "            # Create filenames for output\n",
    "            output_file_simplex = os.path.join(output_dir_simplex, f\"{file[:-4]}_{i}.txt\")\n",
    "            output_file_size_distribution = os.path.join(output_dir_size_distribution, f\"{file[:-4]}_{i}.txt\")\n",
    "\n",
    "            # Write node degrees and size distribution to files\n",
    "            write_simplex_per_node_distribution(output_file_simplex, hypergraph)\n",
    "            write_size_distribution(output_file_size_distribution, hypergraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05641e",
   "metadata": {},
   "source": [
    "# Diffusion / GAN / VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da5a295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hypernetx as hn\n",
    "import random\n",
    "from PIL import Image\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def matrix_to_image(matrix, save_path):\n",
    "    img = Image.fromarray(np.uint8(matrix * 255), 'L')  # 'L' mode for grayscale\n",
    "    img.save(save_path)\n",
    "\n",
    "def pad_matrix(matrix, max_rows, max_cols):\n",
    "    padded_matrix = np.zeros((max_rows, max_cols), dtype=int)\n",
    "    rows, cols = matrix.shape\n",
    "    padded_matrix[:rows, :cols] = matrix\n",
    "    return padded_matrix\n",
    "\n",
    "def shuffle_matrix(matrix):\n",
    "    np.random.shuffle(matrix)\n",
    "    copy = matrix.T\n",
    "    np.random.shuffle(copy)\n",
    "    return copy.T\n",
    "\n",
    "def process_dataset(dataset_name, input_folder, output_base_folder):\n",
    "    # Load the pkl file\n",
    "    with open(os.path.join(input_folder, f'{dataset_name}.pkl'), 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # Extract hypergraphs from 'train'\n",
    "    hypergraphs = data['train']\n",
    "    \n",
    "    # Find the maximum dimensions for padding\n",
    "    max_rows = np.max([len(list(H.nodes)) for H in hypergraphs])\n",
    "    max_cols = np.max([len(list(H.edges)) for H in hypergraphs])\n",
    "    \n",
    "    # Define the folder for saving images\n",
    "    output_folder = os.path.join(output_base_folder, dataset_name, 'train')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Save each shuffled matrix as an image\n",
    "    for i, H in enumerate(hypergraphs):\n",
    "        for j in range(5):  # Save each matrix multiple times with different shuffles\n",
    "            shuffle = shuffle_matrix(H.incidence_matrix().todense())\n",
    "            padded = pad_matrix(shuffle, max_rows, max_cols)\n",
    "            \n",
    "            image_path = os.path.join(output_folder, f'matrix_{i}_{j}.png')\n",
    "            matrix_to_image(padded, image_path)\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = '../data'\n",
    "output_base_folder = 'diffusion/data'\n",
    "\n",
    "# Process all .pkl files in the input folder\n",
    "for file_path in Path(input_folder).glob('*.pkl'):\n",
    "    dataset_name = file_path.stem\n",
    "    process_dataset(dataset_name, input_folder, output_base_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
