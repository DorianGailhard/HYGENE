{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edadadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim, img_shape):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(int(torch.prod(torch.tensor(img_shape))), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, int(torch.prod(torch.tensor(img_shape)))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13d3fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "        self.image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])  # This works for both grayscale and color images\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.folder_path, self.image_files[idx])\n",
    "        image = Image.open(img_name)\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "def load_dataset(folder_name, batch_size=32):\n",
    "    dataset = ImageDataset(folder_path=folder_name)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "003020b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(folder_name, num_epochs=100, latent_dim=100, initial_lr=0.0002, model_save_path='vae_model.pth'):\n",
    "    dataloader = load_dataset(folder_name)\n",
    "    \n",
    "    # Get the shape of the first image to determine the input size for the VAE\n",
    "    first_batch = next(iter(dataloader))\n",
    "    img_shape = first_batch[0].shape\n",
    "    \n",
    "    vae = VAE(latent_dim, img_shape)\n",
    "    \n",
    "    optimizer = optim.Adam(vae.parameters(), lr=initial_lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    # Reconstruction + KL divergence losses summed over all elements and batch\n",
    "    def loss_function(recon_x, x, mu, log_var):\n",
    "        BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, int(torch.prod(torch.tensor(img_shape)))), reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        return BCE + KLD\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i, imgs in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            recon_batch, mu, log_var = vae(imgs)\n",
    "            loss = loss_function(recon_batch, imgs, mu, log_var)\n",
    "            \n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Average loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save({\n",
    "        'vae_state_dict': vae.state_dict(),\n",
    "    }, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6733829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from skimage.filters import threshold_local\n",
    "import numpy as np\n",
    "\n",
    "def adaptive_threshold(tensor, block_size=35, offset=10):\n",
    "    # Convert torch tensor to numpy array\n",
    "    np_image = tensor.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Ensure the image is in the correct range [0, 255]\n",
    "    np_image = (np_image - np_image.min()) / (np_image.max() - np_image.min()) * 255\n",
    "    np_image = np_image.astype(np.uint8)\n",
    "    \n",
    "    # Apply adaptive thresholding\n",
    "    # thresh = threshold_local(np_image, block_size, offset=offset)\n",
    "    binary = np_image > 100\n",
    "    \n",
    "    # Convert back to torch tensor\n",
    "    binary_tensor = torch.from_numpy(binary.astype(np.float32)).unsqueeze(0)\n",
    "    \n",
    "    return binary_tensor\n",
    "\n",
    "def sample_from_vae(model_path, num_samples, latent_dim=100, img_shape=(1, 28, 28), device='cpu'):\n",
    "    vae = VAE(latent_dim, img_shape)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    vae.load_state_dict(checkpoint['vae_state_dict'])\n",
    "    vae.eval()\n",
    "    vae.to(device)\n",
    "    \n",
    "    samples = []\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim, device=device)\n",
    "        gen_imgs = vae.decode(z)\n",
    "        samples = [adaptive_threshold(img.cpu().view(img_shape)) for img in gen_imgs]\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def save_samples(samples, folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        save_image(sample, os.path.join(folder_path, f'sample_{i}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7d0aed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DCGAN for dataset: hypergraphErdosRenyi\n",
      "Epoch [1/100], D_loss: 0.3722, G_loss: 0.6743, LR: 0.000200\n",
      "Epoch [2/100], D_loss: 0.5198, G_loss: 0.8616, LR: 0.000200\n",
      "Epoch [3/100], D_loss: 0.6752, G_loss: 0.3070, LR: 0.000200\n",
      "Epoch [4/100], D_loss: 0.8007, G_loss: 0.2326, LR: 0.000199\n",
      "Epoch [5/100], D_loss: 0.7187, G_loss: 0.2946, LR: 0.000199\n",
      "Epoch [6/100], D_loss: 0.5864, G_loss: 0.9891, LR: 0.000198\n",
      "Epoch [7/100], D_loss: 0.8120, G_loss: 0.9363, LR: 0.000198\n",
      "Epoch [8/100], D_loss: 0.6232, G_loss: 1.6396, LR: 0.000197\n",
      "Epoch [9/100], D_loss: 0.7723, G_loss: 0.3278, LR: 0.000196\n",
      "Epoch [10/100], D_loss: 0.6949, G_loss: 0.5316, LR: 0.000195\n",
      "Epoch [11/100], D_loss: 0.7064, G_loss: 0.6541, LR: 0.000194\n",
      "Epoch [12/100], D_loss: 0.6721, G_loss: 0.5889, LR: 0.000193\n",
      "Epoch [13/100], D_loss: 0.6195, G_loss: 0.8037, LR: 0.000192\n",
      "Epoch [14/100], D_loss: 0.6921, G_loss: 0.6807, LR: 0.000190\n",
      "Epoch [15/100], D_loss: 0.6894, G_loss: 0.9438, LR: 0.000189\n",
      "Epoch [16/100], D_loss: 0.7983, G_loss: 0.3216, LR: 0.000188\n",
      "Epoch [17/100], D_loss: 0.5466, G_loss: 0.9130, LR: 0.000186\n",
      "Epoch [18/100], D_loss: 0.5739, G_loss: 0.8544, LR: 0.000184\n",
      "Epoch [19/100], D_loss: 0.6364, G_loss: 0.6315, LR: 0.000183\n",
      "Epoch [20/100], D_loss: 0.6582, G_loss: 0.8006, LR: 0.000181\n",
      "Epoch [21/100], D_loss: 0.7246, G_loss: 0.6837, LR: 0.000179\n",
      "Epoch [22/100], D_loss: 0.7186, G_loss: 0.6972, LR: 0.000177\n",
      "Epoch [23/100], D_loss: 0.6827, G_loss: 0.6653, LR: 0.000175\n",
      "Epoch [24/100], D_loss: 0.6994, G_loss: 0.5734, LR: 0.000173\n",
      "Epoch [25/100], D_loss: 0.6929, G_loss: 0.6840, LR: 0.000171\n",
      "Epoch [26/100], D_loss: 0.7006, G_loss: 0.7126, LR: 0.000168\n",
      "Epoch [27/100], D_loss: 0.6904, G_loss: 0.7237, LR: 0.000166\n",
      "Epoch [28/100], D_loss: 0.6360, G_loss: 0.7381, LR: 0.000164\n",
      "Epoch [29/100], D_loss: 0.6755, G_loss: 0.7365, LR: 0.000161\n",
      "Epoch [30/100], D_loss: 0.7087, G_loss: 1.0232, LR: 0.000159\n",
      "Epoch [31/100], D_loss: 0.7105, G_loss: 0.6849, LR: 0.000156\n",
      "Epoch [32/100], D_loss: 0.6757, G_loss: 0.6680, LR: 0.000154\n",
      "Epoch [33/100], D_loss: 0.6949, G_loss: 0.5440, LR: 0.000151\n",
      "Epoch [34/100], D_loss: 0.7145, G_loss: 0.6506, LR: 0.000148\n",
      "Epoch [35/100], D_loss: 0.6526, G_loss: 0.7676, LR: 0.000145\n",
      "Epoch [36/100], D_loss: 0.6589, G_loss: 0.7024, LR: 0.000143\n",
      "Epoch [37/100], D_loss: 0.6888, G_loss: 0.8777, LR: 0.000140\n",
      "Epoch [38/100], D_loss: 0.6870, G_loss: 0.6749, LR: 0.000137\n",
      "Epoch [39/100], D_loss: 0.6439, G_loss: 0.7829, LR: 0.000134\n",
      "Epoch [40/100], D_loss: 0.7120, G_loss: 0.6781, LR: 0.000131\n",
      "Epoch [41/100], D_loss: 0.6453, G_loss: 0.7568, LR: 0.000128\n",
      "Epoch [42/100], D_loss: 0.6711, G_loss: 0.6830, LR: 0.000125\n",
      "Epoch [43/100], D_loss: 0.6406, G_loss: 0.7310, LR: 0.000122\n",
      "Epoch [44/100], D_loss: 0.6612, G_loss: 0.6693, LR: 0.000119\n",
      "Epoch [45/100], D_loss: 0.6490, G_loss: 0.8204, LR: 0.000116\n",
      "Epoch [46/100], D_loss: 0.6816, G_loss: 0.6513, LR: 0.000113\n",
      "Epoch [47/100], D_loss: 0.6854, G_loss: 0.6508, LR: 0.000109\n",
      "Epoch [48/100], D_loss: 0.6771, G_loss: 0.7213, LR: 0.000106\n",
      "Epoch [49/100], D_loss: 0.6932, G_loss: 0.6695, LR: 0.000103\n",
      "Epoch [50/100], D_loss: 0.6598, G_loss: 0.6834, LR: 0.000100\n",
      "Epoch [51/100], D_loss: 0.6988, G_loss: 0.6503, LR: 0.000097\n",
      "Epoch [52/100], D_loss: 0.7077, G_loss: 0.7111, LR: 0.000094\n",
      "Epoch [53/100], D_loss: 0.6820, G_loss: 0.8359, LR: 0.000091\n",
      "Epoch [54/100], D_loss: 0.6725, G_loss: 0.6770, LR: 0.000087\n",
      "Epoch [55/100], D_loss: 0.6632, G_loss: 0.6549, LR: 0.000084\n",
      "Epoch [56/100], D_loss: 0.6954, G_loss: 0.7428, LR: 0.000081\n",
      "Epoch [57/100], D_loss: 0.6781, G_loss: 0.6867, LR: 0.000078\n",
      "Epoch [58/100], D_loss: 0.6849, G_loss: 0.7146, LR: 0.000075\n",
      "Epoch [59/100], D_loss: 0.7096, G_loss: 0.7068, LR: 0.000072\n",
      "Epoch [60/100], D_loss: 0.6673, G_loss: 0.7815, LR: 0.000069\n",
      "Epoch [61/100], D_loss: 0.6748, G_loss: 0.7394, LR: 0.000066\n",
      "Epoch [62/100], D_loss: 0.7003, G_loss: 0.6791, LR: 0.000063\n",
      "Epoch [63/100], D_loss: 0.6952, G_loss: 0.6849, LR: 0.000060\n",
      "Epoch [64/100], D_loss: 0.6685, G_loss: 0.6729, LR: 0.000057\n",
      "Epoch [65/100], D_loss: 0.6840, G_loss: 0.7318, LR: 0.000055\n",
      "Epoch [66/100], D_loss: 0.6952, G_loss: 0.7346, LR: 0.000052\n",
      "Epoch [67/100], D_loss: 0.6858, G_loss: 0.7031, LR: 0.000049\n",
      "Epoch [68/100], D_loss: 0.6790, G_loss: 0.7488, LR: 0.000046\n",
      "Epoch [69/100], D_loss: 0.6672, G_loss: 0.7524, LR: 0.000044\n",
      "Epoch [70/100], D_loss: 0.6584, G_loss: 0.6539, LR: 0.000041\n",
      "Epoch [71/100], D_loss: 0.6720, G_loss: 0.7254, LR: 0.000039\n",
      "Epoch [72/100], D_loss: 0.6486, G_loss: 0.7002, LR: 0.000036\n",
      "Epoch [73/100], D_loss: 0.6550, G_loss: 0.7184, LR: 0.000034\n",
      "Epoch [74/100], D_loss: 0.6678, G_loss: 0.7299, LR: 0.000032\n",
      "Epoch [75/100], D_loss: 0.6775, G_loss: 0.7179, LR: 0.000029\n",
      "Epoch [76/100], D_loss: 0.6630, G_loss: 0.7329, LR: 0.000027\n",
      "Epoch [77/100], D_loss: 0.6754, G_loss: 0.7466, LR: 0.000025\n",
      "Epoch [78/100], D_loss: 0.6842, G_loss: 0.7083, LR: 0.000023\n",
      "Epoch [79/100], D_loss: 0.6820, G_loss: 0.6967, LR: 0.000021\n",
      "Epoch [80/100], D_loss: 0.6970, G_loss: 0.6914, LR: 0.000019\n",
      "Epoch [81/100], D_loss: 0.6946, G_loss: 0.6926, LR: 0.000017\n",
      "Epoch [82/100], D_loss: 0.6933, G_loss: 0.6896, LR: 0.000016\n",
      "Epoch [83/100], D_loss: 0.6809, G_loss: 0.6813, LR: 0.000014\n",
      "Epoch [84/100], D_loss: 0.6755, G_loss: 0.7091, LR: 0.000012\n",
      "Epoch [85/100], D_loss: 0.6574, G_loss: 0.7037, LR: 0.000011\n",
      "Epoch [86/100], D_loss: 0.6617, G_loss: 0.7153, LR: 0.000010\n",
      "Epoch [87/100], D_loss: 0.6690, G_loss: 0.7091, LR: 0.000008\n",
      "Epoch [88/100], D_loss: 0.6585, G_loss: 0.7177, LR: 0.000007\n",
      "Epoch [89/100], D_loss: 0.6613, G_loss: 0.7076, LR: 0.000006\n",
      "Epoch [90/100], D_loss: 0.6721, G_loss: 0.7192, LR: 0.000005\n",
      "Epoch [91/100], D_loss: 0.6630, G_loss: 0.7105, LR: 0.000004\n",
      "Epoch [92/100], D_loss: 0.6646, G_loss: 0.7172, LR: 0.000003\n",
      "Epoch [93/100], D_loss: 0.6568, G_loss: 0.7153, LR: 0.000002\n",
      "Epoch [94/100], D_loss: 0.6634, G_loss: 0.7160, LR: 0.000002\n",
      "Epoch [95/100], D_loss: 0.6634, G_loss: 0.7197, LR: 0.000001\n",
      "Epoch [96/100], D_loss: 0.6567, G_loss: 0.7199, LR: 0.000001\n",
      "Epoch [97/100], D_loss: 0.6615, G_loss: 0.7141, LR: 0.000000\n",
      "Epoch [98/100], D_loss: 0.6605, G_loss: 0.7173, LR: 0.000000\n",
      "Epoch [99/100], D_loss: 0.6653, G_loss: 0.7138, LR: 0.000000\n",
      "Epoch [100/100], D_loss: 0.6639, G_loss: 0.7242, LR: 0.000000\n",
      "DCGAN for hypergraphErdosRenyi saved to models/hypergraphErdosRenyi_dcgan.pth\n",
      "Training DCGAN for dataset: hypergraphSBM\n",
      "Epoch [1/100], D_loss: 0.3909, G_loss: 0.6430, LR: 0.000200\n",
      "Epoch [2/100], D_loss: 0.3775, G_loss: 0.7492, LR: 0.000200\n",
      "Epoch [3/100], D_loss: 0.4987, G_loss: 0.4849, LR: 0.000200\n",
      "Epoch [4/100], D_loss: 0.3509, G_loss: 1.0651, LR: 0.000199\n",
      "Epoch [5/100], D_loss: 0.5279, G_loss: 0.5144, LR: 0.000199\n",
      "Epoch [6/100], D_loss: 0.7345, G_loss: 0.5290, LR: 0.000198\n",
      "Epoch [7/100], D_loss: 0.6485, G_loss: 0.8070, LR: 0.000198\n",
      "Epoch [8/100], D_loss: 0.5879, G_loss: 0.5533, LR: 0.000197\n",
      "Epoch [9/100], D_loss: 0.6473, G_loss: 0.9088, LR: 0.000196\n",
      "Epoch [10/100], D_loss: 0.6606, G_loss: 0.7752, LR: 0.000195\n",
      "Epoch [11/100], D_loss: 0.6485, G_loss: 0.5482, LR: 0.000194\n",
      "Epoch [12/100], D_loss: 0.6917, G_loss: 0.7695, LR: 0.000193\n",
      "Epoch [13/100], D_loss: 0.6526, G_loss: 0.7412, LR: 0.000192\n",
      "Epoch [14/100], D_loss: 0.6617, G_loss: 0.5677, LR: 0.000190\n",
      "Epoch [15/100], D_loss: 0.7017, G_loss: 1.0151, LR: 0.000189\n",
      "Epoch [16/100], D_loss: 0.7158, G_loss: 0.5219, LR: 0.000188\n",
      "Epoch [17/100], D_loss: 0.6145, G_loss: 0.6545, LR: 0.000186\n",
      "Epoch [18/100], D_loss: 0.6740, G_loss: 0.7627, LR: 0.000184\n",
      "Epoch [19/100], D_loss: 0.6841, G_loss: 0.8155, LR: 0.000183\n",
      "Epoch [20/100], D_loss: 0.6816, G_loss: 0.6724, LR: 0.000181\n",
      "Epoch [21/100], D_loss: 0.6890, G_loss: 0.6181, LR: 0.000179\n",
      "Epoch [22/100], D_loss: 0.7042, G_loss: 0.8289, LR: 0.000177\n",
      "Epoch [23/100], D_loss: 0.7089, G_loss: 0.7058, LR: 0.000175\n",
      "Epoch [24/100], D_loss: 0.7358, G_loss: 1.1861, LR: 0.000173\n",
      "Epoch [25/100], D_loss: 0.6464, G_loss: 0.6362, LR: 0.000171\n",
      "Epoch [26/100], D_loss: 0.6890, G_loss: 0.9586, LR: 0.000168\n",
      "Epoch [27/100], D_loss: 0.6591, G_loss: 0.6166, LR: 0.000166\n",
      "Epoch [28/100], D_loss: 0.6775, G_loss: 0.6453, LR: 0.000164\n",
      "Epoch [29/100], D_loss: 0.6887, G_loss: 0.6320, LR: 0.000161\n",
      "Epoch [30/100], D_loss: 0.6867, G_loss: 0.6830, LR: 0.000159\n",
      "Epoch [31/100], D_loss: 0.6628, G_loss: 0.6353, LR: 0.000156\n",
      "Epoch [32/100], D_loss: 0.6895, G_loss: 0.8051, LR: 0.000154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100], D_loss: 0.6513, G_loss: 0.6947, LR: 0.000151\n",
      "Epoch [34/100], D_loss: 0.6991, G_loss: 0.6729, LR: 0.000148\n",
      "Epoch [35/100], D_loss: 0.6625, G_loss: 0.6736, LR: 0.000145\n",
      "Epoch [36/100], D_loss: 0.6827, G_loss: 0.6775, LR: 0.000143\n",
      "Epoch [37/100], D_loss: 0.6482, G_loss: 0.7159, LR: 0.000140\n",
      "Epoch [38/100], D_loss: 0.6833, G_loss: 0.6737, LR: 0.000137\n",
      "Epoch [39/100], D_loss: 0.6859, G_loss: 0.7803, LR: 0.000134\n",
      "Epoch [40/100], D_loss: 0.7081, G_loss: 0.6483, LR: 0.000131\n",
      "Epoch [41/100], D_loss: 0.6873, G_loss: 0.6469, LR: 0.000128\n",
      "Epoch [42/100], D_loss: 0.6828, G_loss: 0.8553, LR: 0.000125\n",
      "Epoch [43/100], D_loss: 0.6960, G_loss: 0.7647, LR: 0.000122\n",
      "Epoch [44/100], D_loss: 0.6705, G_loss: 0.6731, LR: 0.000119\n",
      "Epoch [45/100], D_loss: 0.6841, G_loss: 0.7493, LR: 0.000116\n",
      "Epoch [46/100], D_loss: 0.6746, G_loss: 0.7635, LR: 0.000113\n",
      "Epoch [47/100], D_loss: 0.7120, G_loss: 1.1470, LR: 0.000109\n",
      "Epoch [48/100], D_loss: 0.6864, G_loss: 0.7164, LR: 0.000106\n",
      "Epoch [49/100], D_loss: 0.7211, G_loss: 0.9808, LR: 0.000103\n",
      "Epoch [50/100], D_loss: 0.6908, G_loss: 0.6016, LR: 0.000100\n",
      "Epoch [51/100], D_loss: 0.6932, G_loss: 0.6808, LR: 0.000097\n",
      "Epoch [52/100], D_loss: 0.6732, G_loss: 0.6246, LR: 0.000094\n",
      "Epoch [53/100], D_loss: 0.6810, G_loss: 0.6831, LR: 0.000091\n",
      "Epoch [54/100], D_loss: 0.6688, G_loss: 0.6793, LR: 0.000087\n",
      "Epoch [55/100], D_loss: 0.7178, G_loss: 0.4832, LR: 0.000084\n",
      "Epoch [56/100], D_loss: 0.6703, G_loss: 0.7691, LR: 0.000081\n",
      "Epoch [57/100], D_loss: 0.7042, G_loss: 0.6725, LR: 0.000078\n",
      "Epoch [58/100], D_loss: 0.6863, G_loss: 0.7014, LR: 0.000075\n",
      "Epoch [59/100], D_loss: 0.6837, G_loss: 0.6481, LR: 0.000072\n",
      "Epoch [60/100], D_loss: 0.6654, G_loss: 0.7076, LR: 0.000069\n",
      "Epoch [61/100], D_loss: 0.6853, G_loss: 0.6645, LR: 0.000066\n",
      "Epoch [62/100], D_loss: 0.6710, G_loss: 0.7206, LR: 0.000063\n",
      "Epoch [63/100], D_loss: 0.6644, G_loss: 0.8246, LR: 0.000060\n",
      "Epoch [64/100], D_loss: 0.7133, G_loss: 0.6446, LR: 0.000057\n",
      "Epoch [65/100], D_loss: 0.6972, G_loss: 0.7088, LR: 0.000055\n",
      "Epoch [66/100], D_loss: 0.6633, G_loss: 0.6952, LR: 0.000052\n",
      "Epoch [67/100], D_loss: 0.6715, G_loss: 0.7006, LR: 0.000049\n",
      "Epoch [68/100], D_loss: 0.6864, G_loss: 0.7056, LR: 0.000046\n",
      "Epoch [69/100], D_loss: 0.6769, G_loss: 0.7197, LR: 0.000044\n",
      "Epoch [70/100], D_loss: 0.7026, G_loss: 0.7048, LR: 0.000041\n",
      "Epoch [71/100], D_loss: 0.6679, G_loss: 0.7198, LR: 0.000039\n",
      "Epoch [72/100], D_loss: 0.6681, G_loss: 0.7159, LR: 0.000036\n",
      "Epoch [73/100], D_loss: 0.6631, G_loss: 0.7172, LR: 0.000034\n",
      "Epoch [74/100], D_loss: 0.6806, G_loss: 0.6955, LR: 0.000032\n",
      "Epoch [75/100], D_loss: 0.6833, G_loss: 0.6996, LR: 0.000029\n",
      "Epoch [76/100], D_loss: 0.6675, G_loss: 0.7198, LR: 0.000027\n",
      "Epoch [77/100], D_loss: 0.6569, G_loss: 0.7255, LR: 0.000025\n",
      "Epoch [78/100], D_loss: 0.6570, G_loss: 0.7159, LR: 0.000023\n",
      "Epoch [79/100], D_loss: 0.6625, G_loss: 0.7279, LR: 0.000021\n",
      "Epoch [80/100], D_loss: 0.6550, G_loss: 0.7188, LR: 0.000019\n",
      "Epoch [81/100], D_loss: 0.6681, G_loss: 0.7056, LR: 0.000017\n",
      "Epoch [82/100], D_loss: 0.6751, G_loss: 0.6926, LR: 0.000016\n",
      "Epoch [83/100], D_loss: 0.6744, G_loss: 0.6932, LR: 0.000014\n",
      "Epoch [84/100], D_loss: 0.6825, G_loss: 0.6953, LR: 0.000012\n",
      "Epoch [85/100], D_loss: 0.6863, G_loss: 0.7002, LR: 0.000011\n",
      "Epoch [86/100], D_loss: 0.6760, G_loss: 0.7084, LR: 0.000010\n",
      "Epoch [87/100], D_loss: 0.6806, G_loss: 0.7032, LR: 0.000008\n",
      "Epoch [88/100], D_loss: 0.6745, G_loss: 0.7030, LR: 0.000007\n",
      "Epoch [89/100], D_loss: 0.6761, G_loss: 0.7098, LR: 0.000006\n",
      "Epoch [90/100], D_loss: 0.6691, G_loss: 0.7146, LR: 0.000005\n",
      "Epoch [91/100], D_loss: 0.6689, G_loss: 0.7078, LR: 0.000004\n",
      "Epoch [92/100], D_loss: 0.6709, G_loss: 0.7060, LR: 0.000003\n",
      "Epoch [93/100], D_loss: 0.6633, G_loss: 0.7153, LR: 0.000002\n",
      "Epoch [94/100], D_loss: 0.6671, G_loss: 0.7140, LR: 0.000002\n",
      "Epoch [95/100], D_loss: 0.6691, G_loss: 0.7142, LR: 0.000001\n",
      "Epoch [96/100], D_loss: 0.6706, G_loss: 0.7161, LR: 0.000001\n",
      "Epoch [97/100], D_loss: 0.6639, G_loss: 0.7119, LR: 0.000000\n",
      "Epoch [98/100], D_loss: 0.6658, G_loss: 0.7197, LR: 0.000000\n",
      "Epoch [99/100], D_loss: 0.6654, G_loss: 0.7185, LR: 0.000000\n",
      "Epoch [100/100], D_loss: 0.6716, G_loss: 0.7107, LR: 0.000000\n",
      "DCGAN for hypergraphSBM saved to models/hypergraphSBM_dcgan.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "def train_all_datasets(base_data_folder, base_model_folder, num_epochs=100, latent_dim=100, initial_lr=0.0002):\n",
    "    for dataset_folder in Path(base_data_folder).iterdir():\n",
    "        if dataset_folder.is_dir():\n",
    "            dataset_name = dataset_folder.name\n",
    "            train_folder = dataset_folder / 'train'\n",
    "            \n",
    "            if train_folder.exists():\n",
    "                model_save_path = os.path.join(base_model_folder, f'{dataset_name}_vae.pth')\n",
    "                \n",
    "                print(f\"Training VAE for dataset: {dataset_name}\")\n",
    "                \n",
    "                train_vae(\n",
    "                    folder_name=str(train_folder),\n",
    "                    num_epochs=num_epochs,\n",
    "                    latent_dim=latent_dim,\n",
    "                    initial_lr=initial_lr,\n",
    "                    model_save_path=model_save_path\n",
    "                )\n",
    "                print(f\"VAE for {dataset_name} saved to {model_save_path}\")\n",
    "            else:\n",
    "                print(f\"Warning: Train folder not found for {dataset_name}\")\n",
    "\n",
    "# Usage\n",
    "base_data_folder = 'data'\n",
    "base_model_folder = 'models'\n",
    "train_all_datasets(base_data_folder, base_model_folder, num_epochs=100, initial_lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9114354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hypergraphErdosRenyi\n",
      "Image dimensions: 1x32x117\n",
      "Generated samples saved in data/hypergraphErdosRenyi/generated_samples_dcgan\n",
      "Processing dataset: hypergraphSBM\n",
      "Image dimensions: 1x32x104\n",
      "Generated samples saved in data/hypergraphSBM/generated_samples_dcgan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_746089/3026823569.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    }
   ],
   "source": [
    "def load_and_generate_samples(base_model_folder, base_data_folder, num_samples=10, latent_dim=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for model_path in Path(base_model_folder).glob('*dcgan.pth'):\n",
    "        dataset_name = model_path.stem[:-6]  # Remove '_dcgan' from the name\n",
    "        train_folder = Path(base_data_folder) / dataset_name / 'train'\n",
    "        \n",
    "        if not train_folder.exists():\n",
    "            print(f\"Warning: Train folder not found for {dataset_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Get dimensions from the first image in the train folder\n",
    "        first_image_path = next(train_folder.glob('*.*'), None)\n",
    "        if first_image_path is None:\n",
    "            print(f\"Warning: No images found in train folder for {dataset_name}\")\n",
    "            continue\n",
    "        \n",
    "        with Image.open(first_image_path) as img:\n",
    "            channels = len(img.getbands())\n",
    "            width, height = img.size\n",
    "        \n",
    "        print(f\"Processing dataset: {dataset_name}\")\n",
    "        print(f\"Image dimensions: {channels}x{height}x{width}\")\n",
    "        \n",
    "        generated_images = sample_from_gan(model_path, num_samples, latent_dim, img_shape=(1, height, width))\n",
    "        \n",
    "        # Save samples\n",
    "        output_folder = Path(base_data_folder) / dataset_name / 'generated_samples_dcgan'\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        for i, img in enumerate(generated_images):\n",
    "            save_image(img, str(output_folder / f\"sample_{i+1}.png\"), normalize=True)\n",
    "        \n",
    "        print(f\"Generated samples saved in {output_folder}\")\n",
    "\n",
    "# Usage\n",
    "latent_dim=100\n",
    "base_model_folder = 'models'\n",
    "base_data_folder = 'data'\n",
    "load_and_generate_samples(base_model_folder, base_data_folder, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4563d79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3101953\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84c29b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypergraphErdosRenyi\n",
      "Metrics for dataset hypergraphErdosRenyi:\n",
      "NodeNumDiff: 0.0\n",
      "NodeDegreeDistrWasserstein: 56.65078125000001\n",
      "EdgeSizeDistrWasserstein: 15.150107649212696\n",
      "Spectral: 0.4784787119201299\n",
      "CentralityCloseness: 0.41243045405411494\n",
      "CentralityBetweenness: 0.008582975109095349\n",
      "CentralityHarmonic: 55.62403072745771\n",
      "Uniqueness: 1.0\n",
      "Novelty: 1.0\n",
      "\n",
      "==================================================\n",
      "\n",
      "hypergraphEgo\n",
      "Metrics for dataset hypergraphEgo:\n",
      "NodeNumDiff: 24.5\n",
      "NodeDegreeDistrWasserstein: 43.69269389185671\n",
      "EdgeSizeDistrWasserstein: 69.42772737056265\n",
      "Spectral: 0.2543679135663608\n",
      "CentralityCloseness: 0.03370621793060704\n",
      "CentralityBetweenness: 0.0006188258000384554\n",
      "CentralityHarmonic: 20.650962036149703\n",
      "Uniqueness: 1.0\n",
      "Novelty: 1.0\n",
      "ValidEgo: 0.0\n",
      "\n",
      "==================================================\n",
      "\n",
      "hypergraphSBM\n",
      "Metrics for dataset hypergraphSBM:\n",
      "NodeNumDiff: 0.3\n",
      "NodeDegreeDistrWasserstein: 50.26655412066247\n",
      "EdgeSizeDistrWasserstein: 14.860459433040077\n",
      "Spectral: 0.4041841979537313\n",
      "CentralityCloseness: 0.43757615606593847\n",
      "CentralityBetweenness: 0.014774737305912942\n",
      "CentralityHarmonic: 57.49879612059878\n",
      "Uniqueness: 1.0\n",
      "Novelty: 1.0\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b9949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
